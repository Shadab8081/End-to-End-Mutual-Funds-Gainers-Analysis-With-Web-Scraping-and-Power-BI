{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import Required Libraries"
      ],
      "metadata": {
        "id": "yuZZ1pD674Se"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time"
      ],
      "metadata": {
        "id": "lEI6fZnGfaB2"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Define Headers and Base URL"
      ],
      "metadata": {
        "id": "Xn3_omzh79mA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.162 Safari/537.36'\n",
        "}\n",
        "\n",
        "base_url = 'https://finance.yahoo.com/markets/mutualfunds/gainers/?start={}&count=25'"
      ],
      "metadata": {
        "id": "Ps0tpRXqmCTw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Set Column Names and Initialize Variables"
      ],
      "metadata": {
        "id": "BPSNE0ew8AdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['Symbol', 'Name', 'Price', 'Change', 'Change %', '50 Day Avg', '200 Day Avg', '3 Month Return', 'YTD Return', '52 Wk Change %', '52 Wk Range']\n",
        "all_data = []"
      ],
      "metadata": {
        "id": "xbQPO6_HnPst"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Control scraping depth"
      ],
      "metadata": {
        "id": "6m8n6UG18DOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_pages = 340  # limit scraping to 340 pages\n",
        "empty_page_streak = 0  # counter to stop scraping when multiple pages are empty\n",
        "max_empty_pages = 5     # stop after 5 consecutive empty pages"
      ],
      "metadata": {
        "id": "DeW6nQye69JA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Start Scraping Loop"
      ],
      "metadata": {
        "id": "ydmA0iCm8GLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for page in range(0, max_pages * 25, 25):\n",
        "    print(f\"Scraping page with start={page}...\")\n",
        "    url = base_url.format(page)\n",
        "    # Send HTTP request\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=15)\n",
        "        response.raise_for_status()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to fetch page {page // 25 + 1}: {e}\")\n",
        "        continue\n",
        "# Parse HTML content\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    table = soup.find('table')\n",
        "# Handle missing tables(empty pages)\n",
        "    if not table:\n",
        "        print(f\"No table found on page {page // 25 + 1}\")\n",
        "        empty_page_streak += 1\n",
        "        if empty_page_streak >= max_empty_pages:\n",
        "            print(\"Too many empty pages. Stopping scrape.\")\n",
        "            break\n",
        "        continue\n",
        "    else:\n",
        "        empty_page_streak = 0  # reset if we found a table\n",
        "# Extract rows (skip table header)\n",
        "    rows = table.find_all('tr')[1:]  # skip header\n",
        "# Extract data from each row\n",
        "    for row in rows:\n",
        "        cols = row.find_all('td')\n",
        "        data = [col.get_text(strip=True) for col in cols]\n",
        "        if len(data) == len(columns):\n",
        "            all_data.append(data)\n",
        "# Pause between requests to avoid being blocked\n",
        "    time.sleep(1)  # be polite"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Z8FkqAjBnY80",
        "outputId": "8ffe94a0-db03-4aab-dd2c-3d9bc372a380"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page with start=0...\n",
            "Scraping page with start=25...\n",
            "Scraping page with start=50...\n",
            "Scraping page with start=75...\n",
            "Scraping page with start=100...\n",
            "Scraping page with start=125...\n",
            "Scraping page with start=150...\n",
            "Scraping page with start=175...\n",
            "Scraping page with start=200...\n",
            "Scraping page with start=225...\n",
            "Scraping page with start=250...\n",
            "Scraping page with start=275...\n",
            "Scraping page with start=300...\n",
            "Scraping page with start=325...\n",
            "Scraping page with start=350...\n",
            "No table found on page 15\n",
            "Scraping page with start=375...\n",
            "No table found on page 16\n",
            "Scraping page with start=400...\n",
            "No table found on page 17\n",
            "Scraping page with start=425...\n",
            "No table found on page 18\n",
            "Scraping page with start=450...\n",
            "No table found on page 19\n",
            "Too many empty pages. Stopping scrape.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Save Scraped Data to CSV"
      ],
      "metadata": {
        "id": "cIcKB03q8JWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results\n",
        "if all_data:\n",
        "    df = pd.DataFrame(all_data, columns=columns)\n",
        "    df.to_csv(\"yahoo_mutual_fund_gainers.csv\", index=False)\n",
        "    print(\"Scraping completed. Data saved to yahoo_mutual_fund_gainers.csv\")\n",
        "else:\n",
        "    print(\"No data scraped.\")"
      ],
      "metadata": {
        "id": "cwpsraqCnhRA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}